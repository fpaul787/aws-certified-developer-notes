# Amazon S3


## What is AWS S3?
Allows people to store objects (files) in "buckets" (directories). An object is a piece of data like a document,
image, or video that is stored with some metadata in a fl at structure. Object storage provides
that data to applications via application programming interfaces (APIs) over the internet.

#### Object Storage
Applications developed on the cloud often take advantage of object storage’s vast scalability
and metadata characteristics. Object storage solutions like Amazon S3 are ideal for building
modern applications from scratch that require scale and flexibility and can also be used
to import existing data stores for analytics, backup, or archive.

## Amazon Simple Storage Service
Building a web application, which delivers content to users by retrieving data via making
API calls over the internet, is not a diffi cult task with Amazon S3. Amazon Simple Storage
Service (Amazon S3) is storage for the internet. It is a simple storage service that offers
software developers a highly scalable, reliable, and low-latency data storage infrastructure
at low cost.



#### Buckets
A bucket is a container for objects stored in Amazon S3. Every object is contained in a
bucket. You can think of a bucket in traditional terminology similar to a drive or volume.

* Buckets are defined at the region level
* Naming convention
    * No uppercase
    * No underscore
    * 3-63 characters long
    * Not an IP
    * Must start with lowercase letter or number

* A bucket name must consist of a series of one or more labels, with adjacent labels separated
by a single period ( . ).

* A bucket name must contain lowercase letters, numbers, and hyphens.

* Each label must start and end with a lowercase letter or number.

* Bucket names must not be formatted like IP addresses (for example, 1 92.168.5.4 ).
* AWS recommends that you do not use periods ( . ) in bucket names. When using virtual
hosted-style buckets with Secure Sockets Layer (SSL), the SSL wildcard certificate only
matches buckets that do not contain periods. To work around this, use HTTP or write
your own certificate verification logic.

#### Limitations of Buckets
* Do not use buckets as folders, because there is a maximum limit of 100 buckets per
account.

* You cannot create a bucket within another bucket.
* A bucket is owned by the AWS account that created it, and bucket ownership is not
transferable.
* A bucket must be empty before you can delete it.
After a bucket is deleted, that name becomes available to reuse, but the name might not
be available for you to reuse for various reasons, such as someone else taking the name
after you release it when deleting the bucket. If you expect to use same bucket name,
do not delete the bucket.

## AWS S3 - Versioning
Versioning is a means of keeping multiple variants of an object in the same bucket. You can
use versioning to preserve, retrieve, and restore every version of every object stored in your
Amazon S3 bucket, including recovering deleted objects. With versioning, you can easily
recover from both unintended user actions and application failures.
There are several reasons that developers will turn on versioning of files in Amazon S3,
including the following:
* Protecting from accidental deletion
* Recovering an earlier version
* Retrieving deleted objects
Versioning is turned off by default. When you turn on versioning, Amazon S3 will create
new versions of your object every time you overwrite a particular object key. Every time
you update an object with the same key, Amazon S3 will maintain a new version of it.

## S3 Encryption for Objects 
Data protection refers to protecting data while in transit (as it travels to and from Amazon
S3) and at rest (while it is stored on Amazon S3 infrastructure). As a best practice, all sensitive
data stored in Amazon S3 should be encrypted, both at rest and in transit.

#### Encryption Methods
There are 4 methods of encrypting objects in S3
* SSE-S3: encrypts S3 objects using keys handled & managed by AWS
* SSE-C: when you want to manage your own encryption keys
* SSE-KMS: leverage AWS Key Management Service to manage encryption keys
* Client Side Encryption
    

## Server-Side Encryption (SSE)

#### SSE-SE (Amazon S3 managed keys)
You can set an API fl ag or check a box in the AWS
Management Console to have data encrypted before it is written to disk in Amazon S3.
Each object is encrypted with a unique data key. As an additional safeguard, this key is
encrypted with a periodically-rotated master key managed by Amazon S3. AES-256 is used
for both object and master keys. This feature is offered at no additional cost beyond what
you pay for using Amazon S3.

#### SSE-C (Customer-provided keys)
You can use your own encryption key while uploading
an object to Amazon S3. This encryption key is used by Amazon S3 to encrypt your
data using AES-256. After the object is encrypted, the encryption key you supplied is
deleted from the Amazon S3 system that used it to encrypt your data. When you retrieve
this object from Amazon S3, you must provide the same encryption key in your request.
Amazon S3 verifi es that the encryption key matches, decrypts the object, and returns the
object to you. This feature is also offered at no additional cost beyond what you pay for
using Amazon S3.

#### SSE-KMS
You can encrypt your data in Amazon
S3 by defining an AWS KMS master key within your account to encrypt the unique object
key (referred to as a data key ) that will ultimately encrypt your object. 

1. When you upload
your object, a request is sent to AWS KMS to create an object key. AWS KMS generates this
object key and encrypts it using the master key that you specified earlier. 
2. Then, AWS KMS returns this encrypted object key along with the plaintext object key to Amazon S3. The

3. Amazon S3 web server encrypts your object using the plaintext object key and stores the
now encrypted object (with the encrypted object key) and deletes the plaintext object key
from memory.

To retrieve this encrypted object, Amazon S3 sends the encrypted object key to AWS KMS,
which then decrypts the object key using the correct master key and returns the decrypted
(plaintext) object key to Amazon S3. With the plaintext object key, Amazon S3 decrypts
the encrypted object and returns it to you. Unlike SSE-S3 and SSE-C, using SSE-KMS does
incur an additional charge. 

#### Client-Side Encryption
Client-side encryption refers to encrypting your data before sending it to Amazon S3. You
have two options for using data encryption keys.

**Client-Side Master Key**
The first option is to use a client-side master key of your own. When uploading an object,
you provide a client-side master key to the Amazon S3 encryption client (for example,
AmazonS3EncryptionClient when using the AWS SDK for Java). The client uses this master
key only to encrypt the data encryption key that it generates randomly. 

When downloading
an object, the client first downloads the encrypted object from Amazon S3 along
with the metadata. Using the material description in the metadata, the client first determines
which master key to use to decrypt the encrypted data key. Then the client uses that
master key to decrypt the data key and uses it to decrypt the object. The client-side master
key that you provide can be either a symmetric key or a public/private key pair.
The process works as follows:
1. The Amazon S3 encryption client locally generates a one-time-use symmetric key
(also known as a data encryption key or data key ) and uses this data key to encrypt
the data of a single Amazon S3 object (for each object, the client generates a separate
data key).

2. The client encrypts the data encryption key using the master key that you provide.
3. The client uploads the encrypted data key and its material description as part of
the object metadata. The material description helps the client later determine which
client-side master key to use for decryption (when you download the object, the client
decrypts it).
4. The client then uploads the encrypted data to Amazon S3 and also saves the encrypted
data key as object metadata ( x-amz-meta-x-amz-key ) in Amazon S3 by default.

**AWS KMS-Managed Customer Master Key (CMK)**
The second option is to use an AWS KMS managed customer master key (CMK). This process is
similar to the process described earlier for using KMS-SSE, except that it is used for *data at rest*
instead of *data in transit*.

## S3 Security 

#### User based
* IAM policies - which API calls should be allowed for a specific user from IAM
console

#### Resource Based
* Bucket Policies - bucket wide rules from the S3 console - allows cross account
* Object Access Control List (ACL) – finer grain
* Bucket Access Control List (ACL) – less common

#### Networking:
* Supports VPC Endpoints (for instances in VPC without www internet)
#### Logging and Audit:
* S3 access logs can be stored in other S3 bucket
* API calls can be logged in AWS CloudTrail
#### User Security:
* MFA (multi factor authentication) can be required in versioned buckets to
delete objects
* Signed URLs: URLs that are valid only for a limited time (ex: premium video
service for logged in users)

## Cross-Origin Resoure Sharing
Cross-Origin Resource Sharing (CORS) defi nes a way for client web applications that
are loaded in one domain to interact with resources in a different domain. With CORS
support in Amazon S3, you can build client-side web applications with Amazon S3 and
selectively allow cross-origin access to your Amazon S3 resources while avoiding the need
to use a proxy.

#### Scenario
Suppose that you are hosting a website in an Amazon S3 bucket named website on
Amazon S3. Your users load the website endpoint: http://website.s3-website-useast-
1.amazonaws.com .
Your website will use JavaScript on the web pages that are stored in this bucket to be
able to make authenticated GET and PUT requests against the same bucket by using the
Amazon S3 API endpoint for the bucket: website.s3.amazonaws.com .
A browser would normally block JavaScript from allowing those requests, but with
CORS, you can confi gure your bucket to enable cross-origin requests explicitly from
website.s3-website-us-east-1.amazonaws.com.


## S3 Websites
* S3 can host static websites and have them accessible on the www
* The website URL will be:
    * <bucket-name>.s3-website-<AWS-region>.amazonaws.com
    * OR
    * <bucket-name>.s3-website.<AWS-region>.amazonaws.com
* If you get a 403 (Forbidden) error, make sure the bucket policy allows
public reads!
 
## AWS S3 Performance

#### Consistency Model
* Read after write consistency for PUTS of new objects
    * As soon as an object is written, we can retrieve it
    ex: (PUT 200 -> GET 200)
    * This is true, except if we did a GET before to see if the object existed
    ex: (GET 404 -> PUT 200 -> GET 404) – eventually consistent

* Eventual Consistency for DELETES and PUTS of existing objects
    * If we read an object after updating, we might get the older version
    ex: (PUT 200 -> PUT 200 -> GET 200 (might be older version))
    * If we delete an object, we might still be able to retrieve it for a short time
    ex: (DELETE 200 -> GET 200)

## AWS S3 Performance - Key Names Historic fact and current exam
* When you had > 100 TPS (transaction per second), S3 performance could
degrade
* Behind the scene, each object goes to an S3 partition and for the best
performance, we want the highest partition distribution
* In the exam, and historically, it was recommended to have random characters
in front of your key name to optimise performance:
    * <my_bucket>/5r4d_my_folder/my_file1.txt
    * <my_bucket>/a91e_my_folder/my_file2.txt
    * …
* It was recommended **never to use dates to prefix keys**:
    * <my_bucket>/2018_09_09_my_folder/my_file1.txt
    * <my_bucket>/2018_09_10_my_folder/my_file2.txt

#### Upload
* Faster upload of large objects (>=100MB), use multipart upload:
    * parallelizes PUTs for greater throughput
    * maximize your network bandwidth and efficiency
    * decrease time to retry in case a part fails
    * Must use multi-part if object size is greater than 5GB.
* Use CloudFront to cache S3 objects around the world (improves reads)

#### S3 Transfer Acceleration
Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.

Instead of using the public internet to upload objects from Southeast Asia, across the
globe to Northern Virginia, take advantage of the global Amazon content delivery network
(CDN). AWS has edge locations around the world, and you upload your data to the edge
location closest to your location. This way, you are traveling across the AWS network backbone
to your destination region, instead of across the public internet. This option might
give you a significant performance improvement and better network consistency than the
public internet.

* Use Amazon S3 Transfer Acceleration to work with Amazon S3 over
long geographic distances.
* S3 Transfer Acceleration (uses edge locations) – just need to change the
endpoint you write to, not the code.


## Amazon Simple Storage Service Glacier
Amazon Simple Storage Service Glacier (Amazon S3 Glacier) is a secure, durable, and
extremely low-cost storage service for data archiving that offers the same high durability as
Amazon S3. Unlike Amazon S3 Standard’s immediate retrieval times, Amazon S3 Glacier’s
retrieval times run from a few minutes to several hours.
To keep costs low, Amazon S3 Glacier provides three archive access speeds, ranging
from minutes to hours. This allows you to choose an option that will meet your recovery
time objective (RTO) for backups in your disaster recovery plan.
Amazon S3 Glacier can also be used to secure archives that need to be kept due to a compliance
policy. For example, you may need to keep certain records for seven years before
deletion and only need access during an audit. Amazon S3 Glacier allows redundancy in
your files when audits do occur, but at an extremely low cost in exchange for slower access.

## Summary
AWS cloud computing provides a reliable, scalable, and secure place for your data. Cloud
storage is a critical component of cloud computing, holding the information used by applications.
Big Data analytics, data warehouses, Internet of Things, databases, and backup and
archive applications all rely on some form of data storage architecture. Cloud storage is
typically more reliable, scalable, and secure than traditional on-premises storage systems.

AWS offers a complete range of cloud storage services to support both application and archival
compliance requirements. You may choose from object, file, and block storage services and
cloud data migration options to start designing the foundation of your cloud IT environment.


Amazon S3 is a form of object storage that provides a scalable, durable platform to
make data accessible from any internet location, and it allows you to store and access any
type of data over the internet. Amazon S3 is secure, 99.999999999 percent durable, and
scales past tens of trillions of objects.
Amazon S3 Glacier provides extremely low-cost and highly durable object storage for
long-term backup and archiving of any type of data. Amazon S3 Glacier is a solution for
customers who want low-cost storage for infrequently accessed data. It can replace tape,
and assist with compliance in highly regulated organizations.